{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Appootty/Facial_recognition/blob/main/Face_Expression_Detection_jacob_trial_1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsb4kbG3z8sI"
      },
      "source": [
        "# **SUBSCRIBE to the [channel](https://www.youtube.com/user/19daredevill?sub_confirmation=1) to learn cool things**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ssL2LZ0QIv"
      },
      "source": [
        "![Subscribe](https://media3.giphy.com/media/XGILFirobxDWglaUyj/giphy.gif?cid=ecf05e474afdd7ef5fe2c0fa50f87822d705fbb2613a3b5c&rid=giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pocEtO0r4OVI"
      },
      "source": [
        "# Project has the following stages:\n",
        "\n",
        "1.   Collecting Images with Expressions\n",
        "1.   Detecting Faces in the Images and saving them\n",
        "2.   Training our classifier on the Faces\n",
        "1.   Recognition of expression in new Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU6utBLU2OnW"
      },
      "source": [
        "**Clone Repository**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ6y9zfT5bqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a61e63-0961-4797-dd8d-5815793638f7"
      },
      "source": [
        "!git clone https://github.com/misbah4064/facial_expressions.git"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'facial_expressions' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFRxUtt02Sbe"
      },
      "source": [
        "**Creating necessary directories**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTsln3PG677f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a8ef18-5475-40f2-d51d-21696e81ac67"
      },
      "source": [
        "%cd facial_expressions/\n",
        "%mkdir -p data_set/{anger,happy,neutral,sad,surprise}"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/facial_expressions/facial_expressions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy-1cL_q2WcM"
      },
      "source": [
        "**Extracting Images with expressions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOyaXC3d6_59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5864b8-c193-4743-ab4b-d26fd214fa5b"
      },
      "source": [
        "import cv2\n",
        "with open('anger.txt','r') as f:\n",
        "    img = [line.strip() for line in f]\n",
        "for image in img:\n",
        "    loadedImage = cv2.imread(\"images/\"+image)\n",
        "    cv2.imwrite(\"data_set/anger/\"+image,loadedImage)\n",
        "print(\"done writing\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done writing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlxCzrvbwFYZ"
      },
      "source": [
        "%mkdir dataset"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DudXRMR2f3m"
      },
      "source": [
        "# **Step 1 : Creating Data Set of Faces**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdINLeyywaaS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "ded3c85d-3a7c-45f9-bd1d-4fcb7ba10976"
      },
      "source": [
        "import cv2\n",
        "\n",
        "with open('happy.txt','r') as f:\n",
        "    images = [line.strip() for line in f]\n",
        "\n",
        "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# For each Emotion, enter one numeric face id\n",
        "face_id = input('\\n Enter Emotion id end press <return> ==>  ')\n",
        "\n",
        "count = 0\n",
        "\n",
        "for image in images:\n",
        "    img = cv2.imread(\"data_set/happy/\"+image)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "\n",
        "        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
        "        count += 1\n",
        "\n",
        "        # Save the captured image into the datasets folder\n",
        "        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n",
        "\n",
        "print(\"\\n Done creating face data\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Enter Emotion id end press <return> ==>  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-c36bac9f240f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_set/happy/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE1mOVo4xSai"
      },
      "source": [
        "%mkdir trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdOZODKC2nkc"
      },
      "source": [
        "# **Step 2: Training Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH1_BZPAxaHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "aae703bd-404b-49c7-a312-f7573e978cd5"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Path for face image database\n",
        "path = 'dataset'\n",
        "\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\");\n",
        "\n",
        "# function to get the images and label data\n",
        "def getImagesAndLabels(path):\n",
        "\n",
        "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]\n",
        "    faceSamples=[]\n",
        "    ids = []\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "\n",
        "        PIL_img = Image.open(imagePath).convert('L') # convert it to grayscale\n",
        "        img_numpy = np.array(PIL_img,'uint8')\n",
        "\n",
        "        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n",
        "        faces = detector.detectMultiScale(img_numpy)\n",
        "\n",
        "        for (x,y,w,h) in faces:\n",
        "            faceSamples.append(img_numpy[y:y+h,x:x+w])\n",
        "            ids.append(id)\n",
        "\n",
        "    return faceSamples,ids\n",
        "\n",
        "print (\"\\n [INFO] Training faces....\")\n",
        "faces,ids = getImagesAndLabels(path)\n",
        "recognizer.train(faces, np.array(ids))\n",
        "\n",
        "# Save the model into trainer/trainer.yml\n",
        "recognizer.write('trainer/trainer.yml')\n",
        "\n",
        "# Print the numer of Emotions trained and end program\n",
        "print(\"\\n [INFO] {0} Emotions trained. Exiting Program\".format(len(np.unique(ids))))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [INFO] Training faces....\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv_contrib/modules/face/src/lbph_faces.cpp:362: error: (-210:Unsupported format or combination of formats) Empty training data was given. You'll need more than one sample to learn a model. in function 'train'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-acd83d3cdae1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n [INFO] Training faces....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetImagesAndLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Save the model into trainer/trainer.yml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv_contrib/modules/face/src/lbph_faces.cpp:362: error: (-210:Unsupported format or combination of formats) Empty training data was given. You'll need more than one sample to learn a model. in function 'train'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_OC8rZD2sI1"
      },
      "source": [
        "# **Step 3 : Recognition (Testing)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y91jibJxvsn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "00062dda-7b99-4752-a88c-eda0419a0e28"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "recognizer.read('trainer/trainer.yml')\n",
        "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "#iniciate id counter\n",
        "id = 0\n",
        "\n",
        "# Emotions related to ids: example ==> Anger: id=0,  etc\n",
        "names = ['anger', 'happy', 'None', 'None', 'None', 'None']\n",
        "\n",
        "# Initialize and start realtime video capture\n",
        "cam = cv2.VideoCapture(0)\n",
        "cam.set(3, 640) # set video widht\n",
        "cam.set(4, 480) # set video height\n",
        "\n",
        "# Define min window size to be recognized as a face\n",
        "minW = 0.1*cam.get(3)\n",
        "minH = 0.1*cam.get(4)\n",
        "\n",
        "# ret, img =cam.read()\n",
        "img = cv2.imread(\"Appu-4.jpg\")\n",
        "# img = cv2.flip(img, -1) # Flip vertically\n",
        "\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "faces = faceCascade.detectMultiScale(\n",
        "    gray,\n",
        "    scaleFactor = 1.2,\n",
        "    minNeighbors = 5,\n",
        "    minSize = (int(minW), int(minH)),\n",
        "    )\n",
        "\n",
        "for(x,y,w,h) in faces:\n",
        "\n",
        "    cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        "\n",
        "    id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
        "\n",
        "    # Check if confidence is less them 100 ==> \"0\" is perfect match\n",
        "    if (confidence < 100):\n",
        "        id = names[id]\n",
        "        confidence = \"  {0}%\".format(round(100 - confidence))\n",
        "    else:\n",
        "        id = \"unknown\"\n",
        "        confidence = \"  {0}%\".format(round(100 - confidence))\n",
        "\n",
        "    cv2.putText(img, str(id), (x+5,y-5), font, 1, (255,255,255), 2)\n",
        "    cv2.putText(img, str(confidence), (x+5,y+h-5), font, 1, (255,255,0), 1)\n",
        "\n",
        "cv2.imwrite(\"Appu-4_emotion.jpg\",img)\n",
        "\n",
        "print(\"\\n [INFO] Done detecting and Image is saved\")\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv_contrib/modules/face/src/facerec.cpp:61: error: (-2:Unspecified error) File can't be opened for reading! in function 'read'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-e12f845e6a43>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrecognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBPHFaceRecognizer_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainer/trainer.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcascadePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"haarcascade_frontalface_default.xml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfaceCascade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcascadePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv_contrib/modules/face/src/facerec.cpp:61: error: (-2:Unspecified error) File can't be opened for reading! in function 'read'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omzRngO32zd1"
      },
      "source": [
        "# **Display Detected Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDq9tZb9ysHX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f06ceb6b-038f-4d4e-eeaa-5b25568b33bc"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "image = cv2.imread(\"Appu-4_emotion.jpg\")\n",
        "height, width = image.shape[:2]\n",
        "resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18, 10)\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-dbde6cd10ec2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Appu-4_emotion.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}